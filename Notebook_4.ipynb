{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will learn how to create new variables and deal with categorical variable with linear model.\n",
    "Some models are able to deal with categorical variable, you will discover it in the next notebook.\n",
    "Here we will show you how to transform the data to make it usable by a linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We import the usuals packages and the model from sklearn \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"/Users/jeanbaptiste/Downloads/customerLifetimeValue.csv\", sep=\";\")\n",
    "#We take the columns we need for our models and get the underlying matrix\n",
    "X_numeric = dataset[[\"price_first_item_purchased\", \"pages_visited\"]].values\n",
    "#We also take a categorical variable\n",
    "X_categorical = dataset[\"Country\"]\n",
    "#and we create a new feature\n",
    "dataset[\"price/visited_pages\"] = dataset[\"price_first_item_purchased\"] / dataset[\"pages_visited\"]\n",
    "X_new_feature  = dataset[\"price/visited_pages\"].values.reshape((-1, 1))\n",
    "#We binarize the target, all value greater than a given revenue will become positive (1), other negative(0)\n",
    "y = dataset[\"revenue\"].values\n",
    "y[y <= 175] = 0\n",
    "y[y > 175] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "#We fill missing categorical value with \"unknown\"\n",
    "X_categorical.fillna(\"unknown\", inplace=True)\n",
    "my_binarizer = LabelBinarizer()\n",
    "binarized_categories = my_binarizer.fit_transform(X_categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how to convert categorical variables to numeric variables : each category has a column. For all other category we fill value with 0 except for the right one we fill with a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 44.        ,   6.        ,   0.        , ...,   0.        ,\n",
       "          0.        ,   7.33333333],\n",
       "       [117.        ,   5.        ,   0.        , ...,   0.        ,\n",
       "          0.        ,  23.4       ],\n",
       "       [ 44.        ,   5.        ,   0.        , ...,   0.        ,\n",
       "          0.        ,   8.8       ],\n",
       "       ...,\n",
       "       [ 15.5       ,   5.        ,   0.        , ...,   0.        ,\n",
       "          0.        ,   3.1       ],\n",
       "       [ 44.        ,   8.        ,   0.        , ...,   0.        ,\n",
       "          0.        ,   5.5       ],\n",
       "       [ 44.        ,   5.        ,   0.        , ...,   0.        ,\n",
       "          0.        ,   8.8       ]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To avoid computation problems, we need to drop one column of the binarized categories matrix.\n",
    "#All estimated coefficients will be relative to the category we dropped\n",
    "binarized_categories = binarized_categories[:, 1:]\n",
    "#then we concatenate the matrix with the numerical variables\n",
    "X = np.hstack([X_numeric, binarized_categories, X_new_feature])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We create test and train datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1337)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score : 0.773554, test score : 0.770236\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "train_score = roc_auc_score(y_train, model.predict(X_train))\n",
    "test_score = roc_auc_score(y_test, model.predict(X_test))\n",
    "print(\"train score : %f, test score : %f\"%(train_score, test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn : binarize another categorical variable and add it to the model, then compare your score with what we did!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
