{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some models are very interesting to work with because they show how a simple idea can lead to a powerful model.\n",
    "We will here show the K-neighbors model. The idea is to find the K nearest neighbors of a point and mean their label to estimate the value for this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"/Users/jeanbaptiste/Downloads/customerLifetimeValue.csv\", sep=\";\")\n",
    "#We take the columns we need for our models and get the underlying matrix\n",
    "X_numeric = dataset[[\"price_first_item_purchased\", \"pages_visited\"]].values\n",
    "#We also take a categorical variable\n",
    "X_categorical = dataset[\"Country\"]\n",
    "#and we create a new feature\n",
    "dataset[\"price/visited_pages\"] = dataset[\"price_first_item_purchased\"] / dataset[\"pages_visited\"]\n",
    "X_new_feature  = dataset[\"price/visited_pages\"].values.reshape((-1, 1))\n",
    "#We binarize the target, all value greater than a given revenue will become positive (1), other negative(0)\n",
    "y = dataset[\"revenue\"].values\n",
    "y[y <= 175] = 0\n",
    "y[y > 175] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "#We fill missing categorical value with \"unknown\"\n",
    "#like linear model, KNN needs binarized categories\n",
    "X_categorical.fillna(\"unknown\", inplace=True)\n",
    "my_binarizer = LabelBinarizer()\n",
    "binarized_categories = my_binarizer.fit_transform(X_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 44.        ,   6.        ,   0.        , ...,   0.        ,\n",
       "          0.        ,   7.33333333],\n",
       "       [117.        ,   5.        ,   0.        , ...,   0.        ,\n",
       "          0.        ,  23.4       ],\n",
       "       [ 44.        ,   5.        ,   0.        , ...,   0.        ,\n",
       "          0.        ,   8.8       ],\n",
       "       ...,\n",
       "       [ 15.5       ,   5.        ,   0.        , ...,   0.        ,\n",
       "          0.        ,   3.1       ],\n",
       "       [ 44.        ,   8.        ,   0.        , ...,   0.        ,\n",
       "          0.        ,   5.5       ],\n",
       "       [ 44.        ,   5.        ,   0.        , ...,   0.        ,\n",
       "          0.        ,   8.8       ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We join data, but with this model we don't need to drop the first column\n",
    "#then we concatenate the matrix with the numerical variables\n",
    "X = np.hstack([X_numeric, binarized_categories, X_new_feature])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=10, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We create test and train datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1337)\n",
    "#first we try with five neighbors\n",
    "model = KNeighborsClassifier(n_neighbors=4)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score : 0.770252, test score : 0.749126\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "train_score = roc_auc_score(y_train, model.predict(X_train))\n",
    "test_score = roc_auc_score(y_test, model.predict(X_test))\n",
    "print(\"train score : %f, test score : %f\"%(train_score, test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as you can see, the model works well but it require the number of neighbors to select. To define the best number of neighbors, we will test different values to get the one optimizing the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score : 0.725738, test score : 0.691086\n",
      "train score : 0.773477, test score : 0.736759\n",
      "train score : 0.771147, test score : 0.748106\n",
      "train score : 0.772679, test score : 0.739475\n",
      "train score : 0.771405, test score : 0.745620\n",
      "train score : 0.771006, test score : 0.746818\n",
      "train score : 0.768164, test score : 0.747966\n",
      "train score : 0.769715, test score : 0.750417\n",
      "train score : 0.770252, test score : 0.749126\n",
      "train score : 0.768981, test score : 0.753394\n",
      "train score : 0.767429, test score : 0.751369\n",
      "train score : 0.767771, test score : 0.754303\n",
      "train score : 0.770884, test score : 0.754945\n",
      "train score : 0.767276, test score : 0.749748\n",
      "train score : 0.768643, test score : 0.754563\n",
      "train score : 0.764238, test score : 0.750114\n",
      "train score : 0.766601, test score : 0.751879\n",
      "train score : 0.762652, test score : 0.750374\n"
     ]
    }
   ],
   "source": [
    "for n in range(2, 20) :\n",
    "    model = KNeighborsClassifier(n_neighbors=n)\n",
    "    model.fit(X_train, y_train)\n",
    "    predicted_test = model.predict(X_test)\n",
    "    predicted_train = model.predict(X_train)\n",
    "    score_test = roc_auc_score(y_test, predicted_test)\n",
    "    score_train = roc_auc_score(y_train, predicted_train)\n",
    "    print(\"train score : %f, test score : %f\"%(score_train, score_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way to get best number of neighbors is called \"grid search\". Now it's your turn : choose your variables and use K-Fold to predict your target. Use K-Fold to see the difference in results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
